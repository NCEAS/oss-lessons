# Meta-Analaysis

## Introduction

### Zen and the art of scientific synthesis


![](./images/SW-cat.jpg)     


[Fun deck describing a philosophy of synthesis](https://www.slideshare.net/cjlortie/zen-the-art-of-scientific-synthesis)  



[A brief description of the philosophy](http://onlinelibrary.wiley.com/doi/10.1111/oik.03161/full)  


### Rationale

Summary publications on best practices and approaches to meta-analyses and systematic reviews continue to proliferate for natural scientists in particular including a recent [handbook](http://press.princeton.edu/titles/10045.html). Transparency in reporting is still critical and often largely underdeveloped in the original submission of many syntheses in science. Repeatability and delineation of selection criteria are significant advantages of formalized synthesis and just good open science practices (for instance the work of the [Open Science Framework](https://osf.io/tvyxz/wiki/home/). A [PRISMA report](www.prisma-statement.org) and appendix listing all studies including those excluded are important resources and facile contributions to this end. Ideally, the descriptive dataset from the synthesis (studies, criteria and key bibliometric elements of each publication) can also be published *a priori* in an online repository that provides a DOI and cited as a dataset within the manuscript. Editors and referees alike have posited sample size challenges in the peer-review process. There is no magic number in the number of studies or independent instances from within a set of studies that examine the processes or hypothesis in review. However, cursory trends indicate that at least 10–15 studies are needed to warrant a synthesis and that independent instances from a set of studies often begin upwards of 30 tests for the submissions to date. A more productive way to frame meta-analytic sample sizes is the definition of the search. The framework of concepts should ensure that a broad enough set of terms capture both the primary purpose of the review and potentially related terms. Sensitivity analyses and checks of more than one bibliometric search tool are also appropriate. Readership likely also best relates to a balance point within the synthesis that mitigates increasing specialization in the sciences [(trends listed herein](http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0706.2013.00970.x/full) but still provides sufficient evidence to effectively examine the topic at hand. More is not necessarily better either as a very broad synthesis including close to a 1000 studies or more from queries can produce challenges in assigning appropriate ecological context, defining and identifying subgroups, and scope of implications. The purpose of ALL meta-analysis or systematic reviews should nonetheless be explicit. Statement of purpose is more powerful than a question, but it is recommended that these syntheses not go as far as stating that they have the capacity to direct test a hypothesis.  

![](./images/hypothesis.png)  


Typically, these syntheses explore the strength of evidence for a set of hypotheses and predictions and themselves thus have a purpose and outcome. Meta-analyses are often successful at this scale of exploration versus primary prediction or hypothesis testing terms. The meta-level can push testing to examination at larger-scales. This does not preclude critical scientific thinking, and the examination of implications, gaps, or theory development are powerful discovery tools. Systematic review versus meta-analysis is an increasingly common means to get up to speed in any (every) [discipline](http://www.sciencedirect.com/science/article/pii/S0966636215004993).  


![](./images/footwear.png)  


### Terminology

Systematic reviews describe the meta-data of a field of research (who, where, why, how) whilst meta-analyses use the data directly (what was found). As stated previously, both are powerful synthesis tools (Lortie 2014), and the decision to report the literature landscape (systematic review) versus mean efficacy of a treatment or outcome across studies (meta-analysis) depends on both the purpose of the study and the reporting in the literature. Appropriate statistics have been a limited challenge in science for this set of synthesis tools. The rule-of-thumb has been that primary big data, collected identically can sometimes be treated with conventional statistics such as generalized linear models, but data from different studies must be treated with meta-analytics to control for between-study variation. Creativity in terms and concepts has been high at Oikos for all submissions and clearly a process of discovery for each sub-discipline. These discoveries can however be more extensively communicated in all these syntheses because they will inform future efforts and illuminate the extent that semantic development is needed for ecologists as a whole.

![](./images/zen.png)  



### Workflow

**Purpose:** To examine meta-analysis as a tool for scientific synthesis. This workshop will include a workflow from primary literature compilation to meta-analysis.

**To read or not to read**
Meta-analysis is a powerful scientific synthesis tool.
The first step to be able to critically read and assess their relative merits because of their increasing frequency in most domains of evidence-based inquiry. Here is a short ['how-to' read](http://onlinelibrary.wiley.com/doi/10.1002/jrsm.1109/abstract) meta-analyses.

**Learning outcomes:**  
1. To appreciate the value of formal scientific syntheses of primary research literature. 
2. To understand the difference between a systematic review and meta-analysis. 
3. To be able to do a reproducible literature search and filter. 
4. To be able to calculate appropriate effect size measures. 
5. To be able to visualize and contrast with versus between study effect size variation in R (for derived data) and in effect do a meta-analysis.  


![](./images/steps.png)

**Steps:**  
1. Search  
2. Sort  
3. Synthesize  
4. Summarize  
5. Statistics  

**Open science products:** 
Search matrix listing frequencies 
PRISMA report 
A compiled, derived dataset 
Code and workflow summarizing data (i.e. effect sizes and variances) 
Code to visualize and model data (i.e.  meta-stats) 


## Step 1. Search 

![](./images/rey.jpg)     

Identify and list your search tool or resource. This is an evolving landscape and sensitivity is important. Best practices include the following attributes.  
a. document all search instances and record total number of returned hits   
b. explore search term sensitivity and synonyms in detail  
c. test >1 bibliometric resource  
d. do not add publications 'by hand' or haphazardly  
e. do check select publications (seminal and/or top cited) to calibrate search terms  
f. use abstract surveys to ensure you are capturing the correct set of literature  
g. do a search for key key terms by meta-analysis, review, and systematic review to avoid re-inventing the wheel or to ensure you are on the right track  
h. review boolean operators in searching

### Open science product

To document search process, at the minimum generate an 'evidence matrix'. The evidence matrix as a synthesis tool was formally pioneered by the evidence-based synthesis expert for public health [Joseph Lau](https://www.brown.edu/academics/public-health/research/evidence-synthesis-in-health/news/2015-08/joseph-lau-retires). It has changed significant, but it a form of summary table highlighting frequencies of search terms to illuminate the frequency of the *conjunction* of ideas.

Within a contemporary workflow in R, the most facile representation is a table, and there are many excellent packages suitable for tables.

### Case study  

**A search for reearch on positive interactions and the niche in deserts.**  
The niche is a powerful concept in ecology and at times not entirely coupled to local interactions between species. Herein, we review the capacity for these positive plant interactions to expand the niche of subdominant species. We synthesized the available literature using a formalized systematic review by using the Web of Science and associated terms with niche, positive interactions in plants such as facilitation, and deserts.    

  
```{r evidence matrix, warning=FALSE, message=FALSE}
library(tidyverse)
library(DT)

#Search terms####
search.terms <- read_csv("./data/search.terms.csv")
datatable(search.terms)

total.lit <- search.terms %>% filter(search != "final")
total.lit
totals <- sum(total.lit$hits)
totals
final.list <- 53
totals-final.list

#We did the first search for this term set in 2016 then repeated anew in 2017. Luckily, we documented the search in R, repeated very rapidly, and were able to detect differences within the domain for primary publications.
```


### Exercise

Select a topic and explore search terms, concept conjuction, and contrast at least two bibliometric resources. Note, there are also two R packages associated with data scraping the Web of Science - [bibliometrix](https://cran.r-project.org/web/packages/bibliometrix/index.html) and also [selenium](http://ropensci.github.io/RSelenium/). The former in particular is an excellent tool if you expect to do numerous scrapes of Scopus or the Web of Science.


### Conclusions

1. A reproducible search for evidence objects is critical for any synthesis.  
2. A table that shows the conjuction of related concepts and their relative frequency of detection within a bibliometric resource is recommended.  
3. There are many other viable mechanisms to communicate a search process including correlation matrix viz, wordclouds, ontologies with weighting, and conventional count/frequency data viz tools.

### Additional resources

Here is an excellent example of [alt-viz](https://arxiv.org/pdf/1611.02119.pdf).  
If you repeat a search set > 3-6 months later, it is best to match hits to ensure you contrasting the exact same publication sets.  Here is some [code associated with matching DOIs](https://cjlortie.github.io/DOI-matcher/).  
[Here is an excellent publication](http://onlinelibrary.wiley.com/doi/10.1002/jrsm.1120/abstract) turning the search into 10 clear considerations.  
An [alternative](https://cran.r-project.org/web/packages/formattable/vignettes/formattable-data-frame.html) to DT::datatable


## Step 2. Sort  

![](./images/sort.jpg)     

Now the fun and your real training as a synthesis jedi begins (hello to 10,000 hours for mastery). Search your returned hits for appropriate studies that satisfy the criteria that you list a priori. Typically, there are a set of simple assumptions for exclusion criteria for a meta-analysis in evidence-based research. Not a review, includes terms and actually studied the process at hand, and not a duplicate. Those are the three most common reasons for exclusion for the first round of exclusion.  The second round shifts from abstract/overview review and sorting of studies to detailed evidence extraction. Within this round, the most common reason for exclusion is lack of open data. 

### Open science product

The key element to summarize for reproducible science is the criteria. The most typical mechanism to capture the sort workflow is a [PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses)](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1000097) statement/report, and some disciplines have not been as rapid in adopting this synthesis summary tool. A nice description of its importance [here](http://onlinelibrary.wiley.com/doi/10.1890/15.WB.018/full).

### Case study  

**A search for reearch on positive interactions and the niche in deserts.**  
We will continue with the study of niche within desert ecosystems instance and search matrix.


```{r PRIMSA, warning=FALSE, message=FALSE}
library(tidyverse)
library(PRISMAstatement)

#PRISMA####
#use 'prisma' function from PRISMAstatement 
#https://cran.r-project.org/web/packages/PRISMAstatement/vignettes/PRISMA.html
prisma.report <-read_csv("./data/prisma.csv")
prisma.report
exclusions <- prisma.report %>% filter(inclusion == "N") %>% dplyr::select(ID, category)
#dim(exclusions)

prisma(found = 156,
       found_other = 0,
       no_dupes = 53, 
       screened = 53, 
       screen_exclusions = 0, 
       full_text = 53,
       full_text_exclusions = 24, 
       qualitative = 0, 
       quantitative = 29,
       width = 800, height = 800)

#Summary of exclusions
categories <- exclusions %>% group_by(category) %>% tally() %>% arrange(desc(n))
categories

ggplot(categories, aes(category, n)) + geom_bar(stat = "identity") + coord_flip() + ylim(0,10)

```
  

### Exercise

Use your previous search and select a reasonable set of studies to explore the importance of general versus specific exclusion criteria on processing primary research literature as a synthesis substrate. As an important consideration here, please also try the second round, even briefly, for those not immediately excluded by adding another vector in your dataframe indicating whether useable data are available/provided within the publication. For the purposes of this sort step, the data can be a list, table, or even figure that shows means, variances, and sample sizes for every treatment level needed in the synthesis.

### Conclusions

1. A PRISMA statement is similar to a decision tree/workflow diagram and a recommended tool to summarize the synthesis process from primary research objects including publication and datasets (although typically publications).  
2. The sort process is best conceptualized as a two-step process.  
3. The first round of exclusions include reviews, terms listed by not studied, and duplicates.  
4. The second round of exclusions is typically associated with full publication review and included the former two criteria from the previous step but now adds in an estimate of likelihood that data are available for detailed synthesis.  
5. If there are two few studies with available data for synthesis (from within each study), typically defined as < 10 independent studies, then the jedi synthesis researcher can elect to shift to systematic review and summarize the research landscape of studies without **detailed analysis of strength of evidence from within studies**.

### Additional resources

Even if you never want to engage in these forms of synthesis, ensure you primary research publications are available to others by embracing the principles of open science and [effective scientific reporting](http://onlinelibrary.wiley.com/wol1/doi/10.1111/2041-210X.12758/full).  
A [step-by-step guide](http://www.ccace.ed.ac.uk/research/software-resources/systematic-reviews-and-meta-analyses) to the meta versus systematic review decision.  
The PRISMA flow diagram is required by many journals when submitting a systematic review or meta-analysis. However, several journals including PLOS ONE and PeerJ also require the [PRISMA Checklist (available for donload from site)](http://www.prisma-statement.org) as a supplemental file.

## Step 3. Synthesize 

![](./images/synthesize.jpg)     
 
This is a relatively unassuming step. It precedes the 'summarize' step where you really dig in and work on data viz and the prepartion for meta-statistics. Nonetheless, it is a relatively critical step that is includes the following three processes.  
a. extract evidence from the filtered list of eligible primary studies  
b. tabulate the evidence  
c. Data QA/QC and ensure you have what you need to do a meta.

Step 3, synthesize, is akin to collect data within the primary research context. You have designed your experiment, selected the study site and target organism, and planned your sampling effort (and sample sizes). However, you are doing an experiment here but synthesizing other completed experiments with a planned and structure approach. You are in part reviewing every study and in part scraping data from each instance. 

### Open science product

The data from your efforts should be published as an independent data publication. Depending on data type and discipline, select a data repository that assigns a DOI. You have completed a significant synthesis of research by completing this step - even for a total of 10 studies. The mean, variance, and sample size for every level of each treatment from each study should be included in this dataframe and published. It is not uncommon to exclude include the effect size measure because this is derived data.

### Case study

A very [short meta-analysis](https://peerj.com/articles/265/) of a limited set of studies, at the time, for cushion plants in the alpine and arthropods is a useful example. The primary objective was to explore whether cushion plants can faciliate arthropods. We used this synthesis as a means to identify whether research in this domain was a viable PhD topic.  It was not. We published the data on [figshare](https://figshare.com/articles/The_dataset_for_A_global_meta_analytic_contrast_of_cushion_plant_facilitation_of_plants_and_arthropods/1494752). In a second instance of a limited number of studies, we explored the frequency of study of [positive plant interactions on dunes using meta-analysis](https://peerj.com/articles/768/) and also put the data on [figshare](https://figshare.com/articles/A_meta_analytic_dataset_of_plant_facilitation_in_coastal_dune_systems_responses_regions_and_research_gaps/1495510). When the data were more comprehensive, more traditional journals were interested. Here is an example of a meta-analysis of [global change drivers and invasion in grasslands](http://onlinelibrary.wiley.com/doi/10.1111/1365-2745.12236/abstract) with the data on [figshare](https://figshare.com/articles/Dataset_for_Land_management_trumps_the_effects_of_climate_change_and_elevated_CO2_on_grassland_functioning/1112543). All of these data are appropriate for re-analysis, additional syntheses, and updated searches.  

We typically publish biodiversity data with richness and species occurrence data on [knb](https://knb.ecoinformatics.org).


To build the basic dataframe, we code each study with a unique numeric ID. This facilitates connecting different dataframes and tracking. We record location, categorical classifications that describe the study, and mean, var, and n as separate vectors for each treatment and control within the experimental factor set. Trials as usually encoded with the 1.1, 1.2 notation. These are the norms from within the ecological synthesis community only.

The best to appreciate synthesis data are to see synthesis data!!

```{r synthesize, warning=FALSE, message=FALSE}
library(tidyverse)
library(DT)
#Cushion plant meta-analysis#### 
cushions.metadata <- read_csv("./data/cushion-metadata.csv") #we generally keep a short meta-data table in addition to the detailed textbox and descriptions we provide in the open data repositories when we publish
datatable(cushions.metadata)
cushions.data <- read_csv("./data/cushion-data.csv")
datatable(cushions.data) #these data were very diverse and challenging - consequently the open data we shared were the derived data you see here
#View(data) #best way to way to inspect entire dataframe

#Positive interactions on dunes meta-analysis#### 
dune.metadata <- read_csv("./data/dune-metadata.csv")
datatable(dune.metadata)
dunes.data <- read_csv("./data/dune-growthdata.csv")
datatable(dunes.data)

#Grassland and global change meta-analysis####
grassland.metadata <- read_csv("./data/grassland-metadata.csv")
datatable(grassland.metadata)
grassland.data <- read_csv("./data/grassland-data.csv")
datatable(grassland.data)

#Human health meta-analysis comparing Nedocromil sodium with placebo for preventing exercise-induced bronchoconstriction####
bronchoconstriction <- read_csv("./data/bronchocontriction.csv")
datatable(bronchoconstriction) #excellent example of an important meta within evidence-based medicine, note sample sizes and simplicity

```
  

### Exercise

Build a dataframe for your set of studies from the search and sort meta-steps. Use an spreadsheet app but save as a .csv. Extract mean, n, and variance estimates from a total of 7-10 studies. Note whether effect size measures were reported within each study. Record location of study in lat and long.

### Conclusions

1. To be able to use the formal synthesis tool of a systematic review, topological data from each study are needed including location, method, number of species, hypothesis tested, and relative certainty that the primary study is relevant.  
2. To be able to do a meta-analysis, either a reported effect size from within a study or an estimate of the mean treatment versus control effect, variance, and sample sizes are needed.  Regression or correlation coefficients can be used as an effect size measure directly but counts including number of species cannot be used in meta-statistics.  
3. Extract data, build a dataframe, and check data are the three key process for the synthesis step of an open-science workflow for meta-analysis.  
4. The value of an open data set published in a repository cannot be overstated.

### Additional resources

[The Cochrane Collaboration Handbook](http://training.cochrane.org/handbook) is a superb resource for the workflow.   
Chapters 3-5 in this [handbook for ecology and evolution](https://www.jstor.org/stable/j.ctt24hq6n) is also useful.

## Step 4. Summarize  

![](./images/summarize.jpg)     

This is the abridge, condense, and aggregate step in an open science workflow for meta-analysis syntheses. The recap of the data is important to ensure it is correct and coherent. To summarize, it is useful to assess whether the specific set of studies is representative of the purpose of the synthesis. This is primarily a quantitative process, but there is a qualitative element (not in excluding studies or weighting evidence) in applying judgement. Studies that remain for synthesis are not necessarily the 'best' because some studies do not report data, used dramatically different methodologies, or only indirectly support the field of research and as such were not retained in the literature set. The goal of the summarize step is to recap the data and thus simplify the data to ensure it is telling (or about) the right scientific story. Primarily, this process does not model fitting or formal meta-statistics. The data are used to explore the [five Ws](https://en.wikipedia.org/wiki/Five_Ws) at this step using conventional approaches.

### Open science product

The primary open science research product is an 'evidence map' in some form that describes the what, who, where, when, and why of the research topic. Select the strongest and most important aspect to share and consider publishing this as a stand-alone open science product. Fighshare is an excellent repository for figures and provide a DOI.

### Case study  

In many of the global synthesis efforts within the natural sciences, where the research was done and the relative frequency of study is important. [Here](http://www.nature.com/news/sustainability-map-the-evidence-1.18962) is a recent call to action for literal maps of evidence as a powerful and compelling synthesis tool. Evidence maps can take many forms including bubble, box, and relative frequency plots to highlight the important Ws.

![](./images/evidence.map.jpg)  


An important element of the graphic summarizing the data is that it addresses a principle question for the reader, and for you as the research synthesis jedi, the data viz should assesses the quality and fit of the data for the objective of the synthesis. Consequently, counts and data viz are not necessary for every W, but it is recommended that many be explored and some included in the appendices of the paper as a means to provide a topological summary of the dataset.

```{r summarize, warning=FALSE, message=FALSE}
library(tidyverse)
#Niche synthesis example####
#where
data <- read_csv("./data/niche.data.csv")
data
require(maps)
world<-map_data("world")
map<-ggplot() + geom_polygon(data=world, fill="gray50", aes(x=long, y=lat, group=group))
map + geom_point(data=data, aes(x=long, y=lat)) #render a literal map, i.e. evidence map, of where we study the niche in deserts globally

#count up the why we study niche in deserts####  
#why

data.simple <- data %>% group_by(niche.concept, ecosystem) %>% count()

ggplot(na.omit(data.simple), aes(niche.concept, n, fill = ecosystem)) + geom_bar(stat = "identity") + coord_flip() + scale_fill_brewer(palette = "Blues")

```

### Exercise  

Explore the five Ws for your set of studies. To do this, produce an evidence map showing where the research was done, why the research was done, and even cursorily how the research was done (i.e. mensurative of manipulative). To complete the why and how, quickly review a total of 20 studies and code simply to begin. Then use frequency histograms, barplots, or another simple data viz mapping to explore whether this set of studies is representative of the field and corresponds with your expert personal knowledge of this domain. The purpose of the summary to recap and tell the story but also to assess landscape-level data quality. In this process, continue from the previous step to build the data frame and record whether effect size measures were listed in any of the primary studies. A critical component of the how studies were done is includes what was measure as the response because this influences the effect size meausure selected in the next step, statistic analysis.

### Conclusions

1. A summary of synthesis data is a powerful, relatively direct mechanism to provide a sense of the scope of research for a specific topic.  
2. A data summary is a recap of synthesis dataframe that highlights the main idea of the selection of studies.  
3. An excellent indicator of an appropriate synthesis fit is also an open science product.  
4. **Organizing ideas is just as important as organizing data**.  Consider using tools such as ontologies, conceptual frameworks, standard vocabularies, and heuristic idea mapping methodologies.

### Additional resources

[Evidence maps](https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-016-0204-x) can take numerous forms.  

[Conceptual frameworks](http://onlinelibrary.wiley.com/doi/10.1111/geb.12202/full) summarizing the ideas are also an important outcome of the data summary process for synthesis data.  

[Here](http://onlinelibrary.wiley.com/doi/10.1111/j.1466-8238.2011.00713.x/abstract) is another conceptual framework from a less formal synthesis process but is nonetheless a powerful tool for inspiration. In doing these syntheses, other tools such as ontologies, conceptual frameworks, and standardizing the terminology within a field of research are a profound contribution.


## Step 5. Statistics  


![](./images/statistics.jpg)     


The field of meta-statistics is a rich and fascinating set of tools, literature, and discussions. Herein, we direct the practioner to several extremely useful resources associated with this specific topic. To become competent with an open science worflow for meta-analysis, a detailed knowledge of the statistics is of course preferable but not critical. A viable philosophy for the appropriate application of meta-statistics is best defined by the following set of assumptions. (1) Know thy 'derived' data and the limitations of the set of research summarized. This informs the statistics. (2) All statistics are a tool and a means to end - not the end. (3) The formal application of statistics to data from the t-test to the most advanced analysis you can imagine is an exercise in model fitting. A model is a representation of an idea. Period. Statistics are thus one mechanism that provides the evidence-based synthesis researcher with a tool to represent the ideas that stimulated the synthesis. No more, no less. (4) Description of the patterns across a set of studies or datasets is a powerful discovery tool. Do not underestimate the power of... this side of synthesis (or the force). (5) A meta-analysis is not an experiment and does not directly test a hypothesis. It can provide quantitative evidence and probabilities associed with strength of evidence, differences, and consistency and establish a weighted description of the efficacy of different interventions very broadly. (6) Even more profoundly and not uncommonly, meta-analysis as an open science synthesis research tool can highlight research gaps. (7) An ideal starting point is to assume that simple meta-statistics are akin to an ANOVA but for a population of relatively-independent singular data points. This constellation of points can be modeled as fixed or random. More advanced meta-analyses or differences in the type of data simply shift the perspective from ANOVA to reframing as a GLM (same thing) but then slides along to GLMM and this broader family of thinking. Not dissimilar from primary data statistical thinking. The difference is that we do not have one 'data' but 'many representative data'. Very Seussian. There are four primary processes associated with the statistics step of an open science approach to meta-analysis.  

**Key meta-statistical processes**
(a) calculate effect size(s)  
(b) visualize effect size data (typically a forest plot)  
(c) fit a meta-statistical model  
(d) explore heterogeneity and bias  

### Open science product 

A [forest plot](https://en.wikipedia.org/wiki/Forest_plot) is typically the primary open science research product and assumed to be included in some form within the synthesis publication as a key figure. Ideally, contemporary open science researcher should also publicly and openly disseminate the statistical workflow - i.e. share R code illuminating this element of the workflow and ensuring the meta-statistics like the search and sort processes are reproducible.  

### Case study 

The book [Meta-analysis with R](http://www.springer.com/us/book/9783319214153) provides an extensive set of worked examples including data and R code. The packages 'meta' and 'metafor' are also bundled with sample datasets. To promote continuity, here are some of the examples from previous steps provided as an indication of the ease of meta-analysis with R. It is not the implementation of the code that limits us in doing meta-analyses but the access to data. The package meta (and metafor) provide most meta-statistical options as functions or arguments within broader functions for all the data differences and data structures that an environmental or natural science reseacher will encounter.

The pattern of steps you will see below in code chunk include the following:

data (read in and tidy up as needed)
assign meta-model to object
viz
examine model
heterogeneity
bias
sensitivity


```{r statistical model fitting, message=FALSE, warning=FALSE, eval = FALSE}
#library setup####
library(tidyverse)
library(plotrix) #for quick s.e. calculations sometimes needed for data tidy step
library(meta) #nice package for most meta-statistics

#installation as needed
#pkgs <- c("mada", "meta", "metafor", "metasens", "mvmeta", "netmeta", "rmeta", "ellipse") #recommended install for almost every meta-challenge you will encounter, most parsonimous install is 'meta'
#install.packages(pkgs, repos="http://cran.rstudio.com/") #run once per machine not per instance

#R-STEPS REMINDERRRR####
#data
#assign model
#viz
#examine model
#heterogeneity
#bias
#sensitivity

#CASE 1. Effect sizes####
#if reported within primary studies, you could be ready to go already. If not, the package meta provides a wide variety of standard effect size measures provided you have the data.

#data (read, tidy, and structure as needed)
cushions <-read_csv("./data/cushion-data.csv") #dataset that captured effect sizes directly from plant interaction studies
data <- cushions %>% group_by(Study) %>% summarise(mean.Rii = mean(Rii), error = std.error(Rii))

#assign model (typically a nice meta. function from one of several packages such as meta, metafor, or netmeta)
m <- metagen(mean.Rii, error, studlab = Study, data = data) #fit generic meta-analysis to an object

#viz (draw a standard forest plot or metaregression plot)
forest(m) #grid-based graphics so a bit of work to resize

#examine model (print meta-statistical model you fit)
m #By default, the DerSimonian-Laird estimate (1986) is used in the random effects model (method.tau="DL"), however this argument change be changed to the statistical norm for your field or specific data. another popular method in ecology is the Restricted maximum-likelihood estimator (method.tau="REML")

#heterogeneity (inspect above model Qstats)
#This is listed in model output. From 'meta-analysis with r' book for the 'meta' package for instance, Q is the weighted sum of squares about the fixed effect estimate. Large values of Q indicate greater heterogeneity between the individual studies in a meta-analysis, and greater values of the between-study heterogeneity. If p < 0.05, there is 'significant' heterogeneity from study-to-study. Nonetheless in this context, you must contrast the fixed versus random-effects models and the sign of the study-level effect sizes measures to conclude whether the treatment is effective and/or consistent.  

#bias (explore)
funnel(m) #plot to explore distribution of sign and magnitude of the effect size measures, asymmetry suggests bias in the field of research
metabias(m, method="rank") #if p < 0.05, there is likely plot asymmetry
radial(m) #also an excellent means to qualitatively explore bias but explores the variance associated with effect sizes
metabias(m, method = "linreg") # nice quick check of bias but inspecting residuals is more powerful
#the Rosenthal’s fail-safe number is also sometimes reported in the literature and can be calculated using metafor::fsn
#test sensitivity of the effect size measure
#contrast the fixed versus random effect model
#test the importance of the subgroups -- typically this is the MOST important test for bias - sensitivity

#sensitivity (critical step)
#subgroups we could explore include elevation and the measure used to estimate the effect of this potential keystone species
#elevation cofactor/subgroup
data <- cushions %>% group_by(Study, elevation) %>% summarise(mean.Rii = mean(Rii), error = std.error(Rii))
m <- metagen(mean.Rii, error, studlab = Study, byvar = elevation, data = data)
#viz
forest(m) 
#model
m
#heterogeneity
#inspect heterogeneity now using between AND within groups - very important
#bias is being explored using the byvar argument
#funnel(m) #same as whole model plot

#measure cofactor/subgroup
data <- cushions %>% group_by(Study, measure) %>% summarise(mean.Rii = mean(Rii), error = std.error(Rii))
m <- metagen(mean.Rii, error, studlab = Study, byvar = measure, data = data) #fit generic meta-analysis to an object
forest(m, sortvar=measure) 
#viz
forest(m) 
#model
m
#heterogeneity
#inspect heterogeneity now using between AND within groups - very important
#bias epxlored via the byvar argument
#funnel(m) #same as whole model plot

#and yes, you can do meta-statistics like a GLM with interaction terms and also predictions
#also if data structure and the dataframe are consistent, doing sensitivity analyses can be very rapid using the 'update' global function in R
#there is also a 'metareg' function in the meta package do meta-regression if for instance you test elevation in this example using actual elevation of each study in m and not as a category

#One more quick example to highlight the ease of meta-statistics in R

#CASE 2. Continuous response data
#Revisit dune meta-analysis example

#data
dunes.data <- read_csv("./data/dune-growthLRR.csv")

#assign model
m <- metacont(Ne, MEANe, SDe, Nc, MEANc, SDc, studlab = study, data=dunes.data)#metacont is used for continuous data with arguments in this order within call metacont(n.e, mean.e, sd.e, n.c, mean.c, sd.c, studlab = study, data=data)

#viz
forest(m)

#model
m

#heterogeneity
#inspect main model heterogeneity if statistically significant one needs to identify source (i.e. cofactors or subgroups)

#bias
funnel(m)
metabias(m, method="rank") #if p < 0.05, there is likely plot asymmetry
radial(m) #also an excellent means to qualitatively explore bias but explores the variance associated with effect sizes
metabias(m, method = "linreg")

#sensitivity (bias and heterogeneity)
#in this example, region where the dune study was done is likely an important source of heterogeneity

#data
#no need to wrangle

#assign model
m <- update(m, byvar = Region) #use update

#viz
forest(m)

#model
m

#sensitivity
#interpret within versus between heterogeneity and the byvar effect size patterns

```

### Exercise  

Install then load 'meta' package. Work through one generic example using data within the package, from this repo, or from the resource site for the "Meta-analysis with R" book that provides many meta-datasets. Then, use your existing synthesis dataframe, generate an rmarkdown file and work through the basic meta-statistical steps listed above.

### Conclusions

1. There is a global open science workflow for scientific synthesis that culminates with a systematic review or meta-analysis.  
2. There are several clear steps within the meta-statistics step of this larger worflow.  
3. The meta-statistical workflow is just as important as the statistics.  
4. Herein, only the briefest tour of a single package and some function/argument opportunities are shown. There is an immense set of tools available within the contemporary R community for meta-analyses.  
5. This workflow and parsimonous set of steps to meta-statistics provides a reasonable and balanced approach to interpreting the strength of evidence for a set of studies/datasets and can highlight research gaps. More detailed statistics and tests are available but this approach encompasses 95% of of the synthesis studies currently published within the natural sciences.

### Additional resources 

[Meta-analysis with R](http://www.springer.com/us/book/9783319214153) is a stunning resource.  

The [meta](https://cran.r-project.org/web/packages/meta/index.html) vignette is an excellent starting point for an overview of the functions and data in this package.

The [metafor](https://cran.r-project.org/web/packages/metafor/index.html) resources provided by CRAN are similarly useful.

There are many 'how-to' papers published for meta-analysis. [Here](http://onlinelibrary.wiley.com/wol1/doi/10.1348/000711010X502733/full) is nice example.

['forestplot'](https://cran.r-project.org/web/packages/forestplot/forestplot.pdf) is also a nice, well-supported CRAN alt-viz option.

[summary of forest tweaks from meta package](https://rdrr.io/cran/meta/man/forest.html)